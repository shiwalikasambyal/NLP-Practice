{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMaRcSBe8AvD8874QZDcNiR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-tunPprjMv1Y","executionInfo":{"status":"ok","timestamp":1662360891996,"user_tz":-330,"elapsed":919,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}}},"outputs":[],"source":["import nltk"]},{"cell_type":"code","source":["nltk.download('all')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rj_JyB8IORD5","executionInfo":{"status":"ok","timestamp":1662360953026,"user_tz":-330,"elapsed":33902,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"a729d2e8-65d2-4f1e-efd0-4dd4e3ba7128"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["paragraph = \"\"\" Today on 15th March, we gathered in huge numbers to bid farewell to the group of young students after spending almost 12 years in this school. They have been waiting for many years to go outside and join the college and see. Finally, the day has come here to break their long wait and leave the school’s portals to give them a chance to enter a new world of college.As a class teacher of the class 12th standard students, I would like to say something about what I have been feeling about them for 12 years. They deserve to admire me as they all are gentlemen. My dear students, it took twelve long years for us and you to shape you for your bright future. At a time teacher also learn many things together with his students. So I did the same, too; I saw my growing childhood in you. To grow and shape a student, students and teachers must effort and battle together by the same force. \"\"\""],"metadata":{"id":"03GCVgpVOAnh","executionInfo":{"status":"ok","timestamp":1662360957598,"user_tz":-330,"elapsed":366,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#tokenization into sentences\n","sentences = nltk.sent_tokenize(paragraph)\n","print(sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyZTuz-0O9zW","executionInfo":{"status":"ok","timestamp":1662360963464,"user_tz":-330,"elapsed":410,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"ef4e02df-a597-4205-8f6c-bea55ef8365e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[' Today on 15th March, we gathered in huge numbers to bid farewell to the group of young students after spending almost 12 years in this school.', 'They have been waiting for many years to go outside and join the college and see.', 'Finally, the day has come here to break their long wait and leave the school’s portals to give them a chance to enter a new world of college.As a class teacher of the class 12th standard students, I would like to say something about what I have been feeling about them for 12 years.', 'They deserve to admire me as they all are gentlemen.', 'My dear students, it took twelve long years for us and you to shape you for your bright future.', 'At a time teacher also learn many things together with his students.', 'So I did the same, too; I saw my growing childhood in you.', 'To grow and shape a student, students and teachers must effort and battle together by the same force.']\n"]}]},{"cell_type":"code","source":["#tokenization into words\n","words = nltk.word_tokenize(paragraph)\n","print(words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HhrX5i7PW9x","executionInfo":{"status":"ok","timestamp":1662360966065,"user_tz":-330,"elapsed":368,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"5f5caba8-40ae-4887-8ce1-46e1d6f1e292"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Today', 'on', '15th', 'March', ',', 'we', 'gathered', 'in', 'huge', 'numbers', 'to', 'bid', 'farewell', 'to', 'the', 'group', 'of', 'young', 'students', 'after', 'spending', 'almost', '12', 'years', 'in', 'this', 'school', '.', 'They', 'have', 'been', 'waiting', 'for', 'many', 'years', 'to', 'go', 'outside', 'and', 'join', 'the', 'college', 'and', 'see', '.', 'Finally', ',', 'the', 'day', 'has', 'come', 'here', 'to', 'break', 'their', 'long', 'wait', 'and', 'leave', 'the', 'school', '’', 's', 'portals', 'to', 'give', 'them', 'a', 'chance', 'to', 'enter', 'a', 'new', 'world', 'of', 'college.As', 'a', 'class', 'teacher', 'of', 'the', 'class', '12th', 'standard', 'students', ',', 'I', 'would', 'like', 'to', 'say', 'something', 'about', 'what', 'I', 'have', 'been', 'feeling', 'about', 'them', 'for', '12', 'years', '.', 'They', 'deserve', 'to', 'admire', 'me', 'as', 'they', 'all', 'are', 'gentlemen', '.', 'My', 'dear', 'students', ',', 'it', 'took', 'twelve', 'long', 'years', 'for', 'us', 'and', 'you', 'to', 'shape', 'you', 'for', 'your', 'bright', 'future', '.', 'At', 'a', 'time', 'teacher', 'also', 'learn', 'many', 'things', 'together', 'with', 'his', 'students', '.', 'So', 'I', 'did', 'the', 'same', ',', 'too', ';', 'I', 'saw', 'my', 'growing', 'childhood', 'in', 'you', '.', 'To', 'grow', 'and', 'shape', 'a', 'student', ',', 'students', 'and', 'teachers', 'must', 'effort', 'and', 'battle', 'together', 'by', 'the', 'same', 'force', '.']\n"]}]},{"cell_type":"code","source":["# importing important libraries for stemming\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","#stemming\n","sentences = nltk.sent_tokenize(paragraph) # converting into sentences\n","stemmer = PorterStemmer() #creating object\n","for i in range(len(sentences)): #parsing all sentences using for loop\n","  words = nltk.word_tokenize(sentences[i]) # taking words from each sentence one by one\n","  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]  # avoiding those words which are in stop words in english i.e. applying stop word\n","  sentences[i] = ''.join(words) #creating sentences by joining the words that are accepted in above line\n","  print(sentences[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kK3klUIZPtUY","executionInfo":{"status":"ok","timestamp":1662360971138,"user_tz":-330,"elapsed":353,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"1f7c90dc-b715-46a8-8c55-b16dcbb314a8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["today15thmarch,gatherhugenumberbidfarewelgroupyoungstudentspendalmost12yearschool.\n","theywaitmaniyeargooutsidjoincollegsee.\n","final,daycomebreaklongwaitleavschool’portalgivechancenternewworldcollege.aclassteacherclass12thstandardstudent,iwouldlikesaysomethifeel12year.\n","theydeservadmirgentlemen.\n","mydearstudent,tooktwelvlongyearusshapebrightfutur.\n","attimeteacheralsolearnmanithingtogethstudent.\n","soi,;isawgrowchildhood.\n","togrowshapestudent,studentteachermusteffortbattltogethforc.\n"]}]},{"cell_type":"code","source":["# Cleaning the texts\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer   # for lemmetization\n","ps = PorterStemmer() # creating object for stemming\n","wordnet=WordNetLemmatizer()  # creating object for lemmetization\n","sentences = nltk.sent_tokenize(paragraph)\n","corpus = [] # making list for storing sentence after cleaning\n","for i in range(len(sentences)):   #parsing each sentence\n","    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) # removing all commas , dot etc\n","    review = review.lower()  # converting to lower case\n","    review = review.split()   #list of words\n","    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]  # using stemming\n","    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))] # cleaning using lemmetization\n","    review = ' '.join(review)\n","    corpus.append(review)\n","    print(corpus[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fRGq7PjS1u-","executionInfo":{"status":"ok","timestamp":1662360978163,"user_tz":-330,"elapsed":1814,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"517c77e4-9805-40c9-fc2a-dec1c10e938d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["today th march gathered huge number bid farewell group young student spending almost year school\n","waiting many year go outside join college see\n","finally day come break long wait leave school portal give chance enter new world college class teacher class th standard student would like say something feeling year\n","deserve admire gentleman\n","dear student took twelve long year u shape bright future\n","time teacher also learn many thing together student\n","saw growing childhood\n","grow shape student student teacher must effort battle together force\n"]}]},{"cell_type":"code","source":["# Cleaning the texts\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer   # for lemmetization\n","ps = PorterStemmer() # creating object for stemming\n","wordnet=WordNetLemmatizer()  # creating object for lemmetization\n","sentences = nltk.sent_tokenize(paragraph)\n","corpus = [] # making list for storing sentence after cleaning\n","for i in range(len(sentences)):   #parsing each sentence\n","    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) # removing all commas , dot etc\n","    review = review.lower()  # converting to lower case\n","    review = review.split()   #list of words\n","    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]  # using stemming\n","    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))] # cleaning using lemmetization\n","    review = ' '.join(review)\n","    corpus.append(review)\n"],"metadata":{"id":"DxLp75WRWIOP","executionInfo":{"status":"ok","timestamp":1662360983627,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import re\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","ps = PorterStemmer()\n","wordnet=WordNetLemmatizer()\n","sentences = nltk.sent_tokenize(paragraph)\n","corpus = []\n","for i in range(len(sentences)):\n","    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n","    review = review.lower()\n","    review = review.split()\n","    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n","    review = ' '.join(review)\n","    corpus.append(review)\n","    \n","# Creating the TF-IDF model\n","from sklearn.feature_extraction.text import TfidfVectorizer     # importing library tfidvectorizer\n","cv = TfidfVectorizer()         #creating object\n","X = cv.fit_transform(corpus).toarray()   \n","print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7I5gu47JvdBq","executionInfo":{"status":"ok","timestamp":1662360988682,"user_tz":-330,"elapsed":431,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"5f5fb076-752e-43f3-e97d-83e6aac0f53e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.27605954 0.         0.         0.27605954 0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.27605954\n","  0.         0.         0.         0.         0.27605954 0.\n","  0.         0.         0.27605954 0.         0.         0.27605954\n","  0.         0.         0.         0.         0.         0.\n","  0.27605954 0.         0.         0.27605954 0.         0.\n","  0.         0.         0.23135944 0.         0.         0.\n","  0.27605954 0.         0.15494411 0.         0.23135944 0.\n","  0.         0.27605954 0.         0.         0.         0.\n","  0.         0.         0.         0.17504397 0.27605954]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.3212275  0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.38329068 0.         0.         0.         0.\n","  0.38329068 0.         0.         0.         0.         0.3212275\n","  0.         0.         0.         0.         0.38329068 0.\n","  0.         0.         0.         0.38329068 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.38329068 0.         0.         0.24303715 0.        ]\n"," [0.         0.         0.         0.         0.         0.19592938\n","  0.         0.19592938 0.         0.39185875 0.16420411 0.19592938\n","  0.19592938 0.         0.         0.         0.19592938 0.\n","  0.19592938 0.19592938 0.         0.         0.         0.\n","  0.19592938 0.         0.         0.         0.         0.\n","  0.         0.         0.19592938 0.19592938 0.16420411 0.\n","  0.         0.         0.19592938 0.         0.         0.19592938\n","  0.         0.19592938 0.16420411 0.         0.         0.19592938\n","  0.         0.19592938 0.10996941 0.14169467 0.16420411 0.\n","  0.         0.         0.         0.         0.         0.19592938\n","  0.         0.19592938 0.19592938 0.124235   0.        ]\n"," [0.57735027 0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.57735027 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.57735027\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.37471763 0.         0.         0.         0.         0.\n","  0.         0.37471763 0.         0.         0.         0.\n","  0.         0.         0.         0.37471763 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.31404262 0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.31404262 0.\n","  0.         0.         0.210318   0.         0.         0.\n","  0.         0.         0.         0.37471763 0.37471763 0.\n","  0.         0.         0.         0.23760114 0.        ]\n"," [0.         0.         0.40023118 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.40023118 0.         0.         0.         0.33542497\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.22463801 0.28944422 0.         0.40023118\n","  0.40023118 0.         0.33542497 0.         0.         0.\n","  0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.57735027 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.57735027 0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.57735027 0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.34947402 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.34947402 0.         0.\n","  0.         0.         0.34947402 0.         0.         0.\n","  0.         0.         0.         0.34947402 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.34947402 0.         0.         0.         0.\n","  0.         0.         0.         0.         0.29288651 0.\n","  0.         0.         0.39229901 0.25273702 0.         0.\n","  0.         0.         0.29288651 0.         0.         0.\n","  0.         0.         0.         0.         0.        ]]\n"]}]},{"cell_type":"code","source":["!pip install gensim # for implementing word2vec"],"metadata":{"id":"DuRBvUIXvx4Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662361411455,"user_tz":-330,"elapsed":3253,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"5975c916-6c4c-4545-894b-637623d587ac"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from gensim.models import Word2Vec"],"metadata":{"id":"R-pUMiwLdnpQ","executionInfo":{"status":"ok","timestamp":1662361471109,"user_tz":-330,"elapsed":367,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Preprocessing the data\n","text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n","text = re.sub(r'\\s+',' ',text)\n","text = text.lower()\n","text = re.sub(r'\\d',' ',text)\n","text = re.sub(r'\\s+',' ',text)\n","\n","# Preparing the dataset\n","sentences = nltk.sent_tokenize(text)\n","\n","sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","\n","for i in range(len(sentences)):\n","    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n","# Training the Word2Vec model\n","model = Word2Vec(sentences, min_count=1)     # mincount is used to specify which world to include , it means if the count of any word is 1 then include the word else neglect it\n","\n","\n","words = model.wv.vocab  #used to find vocabulary words obtained in this word2vec model\n","print(words)\n","# Finding Word Vectors\n","vector = model.wv['students'] #checking the vectors obtained for any particular word like her we see vector of student\n","print(vector)\n","\n","# Most similar words\n","similar = model.wv.most_similar('students')  # checking the words that are similar to the word student\n","print(similar)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILOZToUSd2l2","executionInfo":{"status":"ok","timestamp":1662362636065,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shiwalika Sambyal","userId":"09209430205248389571"}},"outputId":"5d745d3c-03c2-4a19-8033-ab5d479398a4"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"]},{"output_type":"stream","name":"stdout","text":["{'today': <gensim.models.keyedvectors.Vocab object at 0x7f841f306490>, 'th': <gensim.models.keyedvectors.Vocab object at 0x7f841f306910>, 'march': <gensim.models.keyedvectors.Vocab object at 0x7f841f306150>, ',': <gensim.models.keyedvectors.Vocab object at 0x7f841f306750>, 'gathered': <gensim.models.keyedvectors.Vocab object at 0x7f841f3062d0>, 'huge': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5250>, 'numbers': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5610>, 'bid': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5190>, 'farewell': <gensim.models.keyedvectors.Vocab object at 0x7f841f306f50>, 'group': <gensim.models.keyedvectors.Vocab object at 0x7f841f306450>, 'young': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5710>, 'students': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d51d0>, 'spending': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5090>, 'almost': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5150>, 'years': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d56d0>, 'school': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5a10>, '.': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5410>, 'waiting': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d50d0>, 'many': <gensim.models.keyedvectors.Vocab object at 0x7f841f2d5fd0>, 'go': <gensim.models.keyedvectors.Vocab object at 0x7f841f28a0d0>, 'outside': <gensim.models.keyedvectors.Vocab object at 0x7f841f305c50>, 'join': <gensim.models.keyedvectors.Vocab object at 0x7f8420bacd50>, 'college': <gensim.models.keyedvectors.Vocab object at 0x7f8421c243d0>, 'see': <gensim.models.keyedvectors.Vocab object at 0x7f8421766fd0>, 'finally': <gensim.models.keyedvectors.Vocab object at 0x7f8421766b50>, 'day': <gensim.models.keyedvectors.Vocab object at 0x7f8423bdbc90>, 'come': <gensim.models.keyedvectors.Vocab object at 0x7f8423bdbdd0>, 'break': <gensim.models.keyedvectors.Vocab object at 0x7f8423bdbd10>, 'long': <gensim.models.keyedvectors.Vocab object at 0x7f8423bdbc10>, 'wait': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54450>, 'leave': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54b10>, '’': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54a50>, 'portals': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54cd0>, 'give': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54810>, 'chance': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54dd0>, 'enter': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54950>, 'new': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54ed0>, 'world': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54110>, 'college.as': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54a90>, 'class': <gensim.models.keyedvectors.Vocab object at 0x7f8420b54ad0>, 'teacher': <gensim.models.keyedvectors.Vocab object at 0x7f8420ba4590>, 'standard': <gensim.models.keyedvectors.Vocab object at 0x7f8420ba45d0>, 'would': <gensim.models.keyedvectors.Vocab object at 0x7f84285bdfd0>, 'like': <gensim.models.keyedvectors.Vocab object at 0x7f841f284210>, 'say': <gensim.models.keyedvectors.Vocab object at 0x7f841f284dd0>, 'something': <gensim.models.keyedvectors.Vocab object at 0x7f841f284490>, 'feeling': <gensim.models.keyedvectors.Vocab object at 0x7f841f284f10>, 'deserve': <gensim.models.keyedvectors.Vocab object at 0x7f841f284a10>, 'admire': <gensim.models.keyedvectors.Vocab object at 0x7f841f284690>, 'gentlemen': <gensim.models.keyedvectors.Vocab object at 0x7f841f284910>, 'dear': <gensim.models.keyedvectors.Vocab object at 0x7f841f284850>, 'took': <gensim.models.keyedvectors.Vocab object at 0x7f841f284050>, 'twelve': <gensim.models.keyedvectors.Vocab object at 0x7f841f2843d0>, 'us': <gensim.models.keyedvectors.Vocab object at 0x7f841f284150>, 'shape': <gensim.models.keyedvectors.Vocab object at 0x7f841f284310>, 'bright': <gensim.models.keyedvectors.Vocab object at 0x7f841f284f50>, 'future': <gensim.models.keyedvectors.Vocab object at 0x7f841f284550>, 'time': <gensim.models.keyedvectors.Vocab object at 0x7f841f2849d0>, 'also': <gensim.models.keyedvectors.Vocab object at 0x7f841f284c10>, 'learn': <gensim.models.keyedvectors.Vocab object at 0x7f841f284e90>, 'things': <gensim.models.keyedvectors.Vocab object at 0x7f841f2848d0>, 'together': <gensim.models.keyedvectors.Vocab object at 0x7f841f284e10>, ';': <gensim.models.keyedvectors.Vocab object at 0x7f841f284410>, 'saw': <gensim.models.keyedvectors.Vocab object at 0x7f841f284190>, 'growing': <gensim.models.keyedvectors.Vocab object at 0x7f841f284d10>, 'childhood': <gensim.models.keyedvectors.Vocab object at 0x7f841f284f90>, 'grow': <gensim.models.keyedvectors.Vocab object at 0x7f841f284950>, 'student': <gensim.models.keyedvectors.Vocab object at 0x7f841f2844d0>, 'teachers': <gensim.models.keyedvectors.Vocab object at 0x7f841f284c50>, 'must': <gensim.models.keyedvectors.Vocab object at 0x7f841f284290>, 'effort': <gensim.models.keyedvectors.Vocab object at 0x7f842176a050>, 'battle': <gensim.models.keyedvectors.Vocab object at 0x7f841f2f98d0>, 'force': <gensim.models.keyedvectors.Vocab object at 0x7f841f2f99d0>}\n","[-1.5731852e-03 -2.0606527e-03  2.7815502e-03 -9.1812428e-04\n","  7.0320565e-04 -1.0059401e-04 -1.7083082e-03  4.0658154e-03\n","  3.7216921e-03  4.4080028e-03  2.3309612e-03  4.1253292e-03\n"," -3.7987367e-03 -2.7311950e-03  5.4641691e-04 -4.7926079e-03\n"," -2.0867698e-03  2.7117131e-03 -4.7727660e-03  3.0627255e-03\n","  3.6586232e-03 -4.4207503e-03  5.4974563e-04 -7.2641898e-04\n","  2.8340339e-03  4.7508706e-03 -3.0328063e-03 -1.7434764e-03\n"," -1.2712672e-03 -1.0602244e-03 -4.0388922e-03 -1.2831234e-04\n","  4.3343483e-03  1.2517933e-04  3.9273668e-03  3.1175008e-03\n"," -1.8774376e-04 -2.8401644e-03 -4.5441412e-03  2.7596531e-03\n","  3.4766641e-04 -1.9655991e-03 -1.2843604e-05 -4.2123171e-03\n"," -4.0055229e-03 -1.2053939e-03  3.4444118e-03 -2.8061250e-03\n","  3.5871766e-03 -3.7481803e-03  4.6564595e-04 -3.3603271e-03\n"," -2.5837030e-03  1.9839411e-03  2.6332953e-03 -1.3758400e-05\n","  2.2763934e-03  3.9836005e-03 -3.3831897e-03  4.7481549e-03\n"," -2.6392366e-03 -3.6226393e-04 -9.1290625e-04  7.3470309e-04\n"," -2.3066152e-03  4.7482522e-03  3.3880174e-03 -4.7566224e-04\n","  2.6518416e-03  4.7856653e-03  4.1744518e-03  1.5303651e-03\n"," -4.4804281e-03  2.7692763e-03 -4.0301867e-03  7.5694389e-04\n"," -2.5294845e-03 -7.8071683e-04  2.7982204e-03  6.6224980e-04\n","  1.9981947e-03 -7.6733180e-04 -2.7671622e-03 -1.1611502e-03\n","  4.1271853e-03  4.9263290e-03  3.6493759e-03 -4.3589510e-03\n","  4.7887708e-03 -1.7111474e-03  2.5021590e-03 -4.7042482e-03\n"," -2.2272428e-03 -3.4569351e-03 -3.0708171e-03 -3.4798430e-03\n"," -3.0979405e-03  3.3307101e-03 -7.5565267e-04 -4.7951615e-03]\n","[('force', 0.20584248006343842), ('also', 0.18830189108848572), ('together', 0.17445459961891174), ('teachers', 0.17011809349060059), ('portals', 0.149629145860672), ('outside', 0.1179957389831543), ('something', 0.11142560839653015), ('us', 0.11072582006454468), ('come', 0.10199811309576035), ('teacher', 0.09772650897502899)]\n"]}]}]}